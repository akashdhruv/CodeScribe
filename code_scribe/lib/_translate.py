# Prompt engineering for building diffusion stencils for constant and variable coefficient equation

# Import libraries
import re
import os, sys, toml, importlib

from typing import Optional
from alive_progress import alive_bar


class OpenAIModel:
    def __init__(self):
        openai = importlib.import_module("openai")
        self.pipeline = openai.OpenAI()
        self.outputs = 1
        self.max_tokens = 4096

    def chat(self, chat_template):
        # We use the Chat Completion endpoint for chat like inputs
        response = self.pipeline.chat.completions.create(
            # model used here is ChatGPT
            # You can use all these models for this endpoint:
            # gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314,
            # gpt-3.5-turbo, gpt-3.5-turbo-0301
            model="gpt-3.5-turbo",
            messages=chat_template,
            # max_tokens generated by the AI model
            # maximu value can be 4096 tokens for "gpt-3.5-turbo"
            max_tokens=self.max_tokens,
            # number of output variations to be generated by AI model
            n=self.outputs,
        )

        return response.choices[0].message.content


class TFModel:
    def __init__(self, checkpoint_dir):
        transformers = importlib.import_module("transformers")
        torch = importlib.import_module("torch")

        self.tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint_dir)
        self.pipeline = transformers.pipeline(
            "text-generation",
            model=checkpoint_dir,
            # torch_dtype=torch.float16,
            device=-1,
        )

        self.max_new_tokens = 4096
        self.batch_size = 8
        self.max_length = None

    def chat(self, chat_template):

        results = self.pipeline(
            chat_template,
            max_new_tokens=self.max_new_tokens,
            max_length=self.max_length,
            batch_size=self.batch_size,
            # temperature=temperature,
            # top_p=top_p,
            # do_sample=True,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=50256,
        )

        return results["generated_text"][-1]["content"]


def prompt_translate(mapping, prompt, model=None, save_prompts=False):
    """
    perform translation using prompts and the supplied model.
    """

    neural_model = None

    if model:
        print("Starting neural conversion process")

        if os.path.exists(model):
            neural_model = TFModel(model)

        elif model.lower() == "openai":
            neural_model = OpenAIModel()

        else:
            raise ValueError(f"{model} not available")

    if save_prompts:
        print("Saving custom prompts per file")

    chat_template = toml.load(prompt)["chat"]

    with alive_bar(len(mapping[0]), bar="blocks") as bar:

        for fsource, csource, finterface, cdraft, promptfile in zip(
            mapping[0], mapping[1], mapping[2], mapping[3], mapping[4]
        ):

            bar.text(fsource)
            bar()

            if not os.path.isfile(csource) or save_prompts:
                cached_prompt = chat_template[-1]["content"]

                with open(fsource, "r") as sfile:
                    is_comment = False
                    source_code = []

                    for line in sfile.readlines():
                        is_comment = False

                        if line.strip().lower().startswith(("c", "!!", "!")) and (
                            not line.strip().lower().startswith(("complex"))
                        ):
                            is_comment = True

                        if not is_comment:
                            source_code.append(line)

                    if source_code:
                        chat_template[-1]["content"] += (
                            "\n" + "<source>\n" + "".join(source_code) + "</source>"
                        )

                if os.path.isfile(cdraft):

                    draft_code = []
                    with open(cdraft) as dfile:
                        for line in dfile.readlines():
                            draft_code.append(line)

                        if draft_code:
                            chat_template[-1]["content"] += (
                                "\n\n" + "<draft>\n" + "".join(draft_code) + "</draft>"
                            )

                if save_prompts:
                    with open(promptfile, "w") as pdest:
                        for instance in chat_template:
                            pdest.write("[[chat]]\n")
                            pdest.write(f'role = "{instance["role"]}"\n')
                            pdest.write(f'content = """\n{instance["content"]}"""\n\n')
                    print(f"Generated prompt file for LLM consumption {promptfile}")

                if neural_model:
                    result = neural_model.chat(chat_template)

                    with open(csource, "w") as cdest, open(finterface, "w") as fdest:

                        csource = re.search(
                            r"<csource>(.*?)</csource>", result, re.DOTALL
                        )
                        fsource = re.search(
                            r"<fsource>(.*?)</fsource>", result, re.DOTALL
                        )

                        if csource:
                            cdest.write(csource.group(1))
                        else:
                            cdest.write(result)

                        if fsource:
                            fdest.write(fsource.group(1))

                chat_template[-1]["content"] = cached_prompt

            else:
                continue
